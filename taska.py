# -*- coding: utf-8 -*-
"""TASKA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yUKduw1DoDp56shkSBMUmtUfpS15qPoK
"""

import pandas as pd
from google.colab import files
import io
#TASK A 1.1
#Upload the file from your computer
uploaded = files.upload()
#Choose motor_insurance_recovery.csv when prompted

#Read the uploaded CSV into a DataFrame
file_name = list(uploaded.keys())[0]  # should be 'motor_insurance_recovery.csv'
df = pd.read_csv(io.BytesIO(uploaded[file_name]))

#Check
print("Dataset shape (rows, columns):", df.shape)
display(df.head())

#Displaying basic info about the dataset.
print("- Dataset shape (rows, columns) -")
print(df.shape)

print("\n- Basic information about the dataset- ")
df.info()

#Showing columns with missing values:
print("\n- Columns with missing values: - ")
missing_counts = df.isna().sum()
missing_counts = missing_counts[missing_counts > 0].sort_values(ascending=False)

print(f"{'Column':35} {'Null Cells':12} {'Data Type'}")
print("-" * 60)

for col, count in missing_counts.items():
    dtype = df[col].dtype
    print(f"{col:35} {str(count):12} {dtype}")

# List of numeric columns  exluding binary columns

numeric_cols = [
     'client_id',
    'claim_value_category',
    'injury_duration_days',
    'work_absence_duration_days',
    'defendant_title_code',
    'claimant_age_at_incident',
    'rental_vehicle_expense',
    'home_care_services_cost',
    'employment_disadvantage_compensation',
    'career_satisfaction_loss_compensation',
    'asset_utility_loss_compensation',
    'medical_treatment_costs',
    'general_damages',
    'insurance_deductible_amount',
    'total_claim_amount',
    'claimant_current_age_years',
    'medical_attention_delay_days'
]

# Summary statistics for numeric columns
print("- Summary Statistics for Numerical Columns -")
display(df[numeric_cols].describe())

"""Summary statistics reveal that various numerical variables display large variability, with evident high-end outliers.For instance, total claim amounts are much higher than the 75th percentile (£26,504), ranging from £1,792 to £68,162.Similarly, general damages exhibit high levels (max £56,096), indicating a tiny number of severe and costly injury claims.  A significant right-skew is also evident in the cost of medical care, with a high of £4,465 and a median of £355. In insurance recovery datasets, where a small percentage of claims result in disproportionately high costs, these outliers are common and, if not handled properly, can have a big impact on our model."""

#frequency counts and percentages for key categorical variables :

categorical_cols = [
    'claim_value_category',
    'car_damage_severity',
    'liability_type',
    'emergency_services_attended',
    'liability_admission_status',
    'rehabilitation_program_types',
    'emergency_services_attended',
    'claims_resolution_closure_code'
]
for col in categorical_cols:
    print(f"\n=== {col} ===")
    counts = df[col].value_counts(dropna=False)
    percentages = df[col].value_counts(normalize=True, dropna=False) * 100

    summary = pd.DataFrame({
        'Count': counts,
        'Percent': percentages.round(2)
    })

    print(summary)

"""The majority of claims, as shown by the categorization analysis, fall into lower-value categories; over 94% of all instances fall into claim value categories 1 and 2, suggesting that lower-cost motor claims predominate in the dataset.  The dataset seems to have a balanced mix of collision types rather than being biased toward light or heavy damage, as seen by the fairly equal distribution of damage severity across minor, moderate, and severe categories (each at around 30%).  Nearly eighty-five percent of incidents required emergency services to respond. The most common reaction was an ambulance and police officers responding together. This shows that plenty of incidents were important enough to need emergency assistance.  Every claim leads to compensation because liability is acknowledged in every instance; there are never disputes.  With numerous minor, distinct combinations that need to be cleaned up or grouped, the data from rehabilitation programs is dispersed and dominated by "Unknown."  Although a sizable percentage of closure codes (E01–E04) are missing, the distribution of these codes is generally uniform, suggesting that some claim outcomes might not be fully documented."""

import ast

# Converting string sets to real sets
df['injury_type_classifications'] = df['injury_type_classifications'].apply(
    lambda x: ast.literal_eval(x) if isinstance(x, str) else x
)

# Explode injury types
injury_exploded = df.explode('injury_type_classifications')

# Frequencies and percentages
print("\n - injury_type_classifications -")

counts = injury_exploded['injury_type_classifications'].value_counts(dropna=False)
percentages = injury_exploded['injury_type_classifications'].value_counts(normalize=True, dropna=False) * 100

summary = pd.DataFrame({
    'Count': counts,
    'Percent': percentages.round(2)
})

print(summary)

"""The most frequent injury type (27%) in motor vehicle incidents is whiplash.  Internal, soft-tissue, limb, spine/back, and other injuries all occur in lesser but comparable percentages (5–8%), suggesting a wide range of injury types across the dataset."""

import pandas as pd
import numpy as np
import ast

def clean_motor_insurance_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    Task A – 1.2 Data Quality Assessment & Cleaning.

    Tasks done:
      1. Treat empty strings as NaN.
      2. Remove exact duplicate rows.
      3. Normalise categorical columns (strip whitespace, lowercase).
      4. Convert selected columns to numeric types.
      5. Basic range checks (e.g. remove negative costs/durations, unrealistic ages).
      6. Convert set-like columns (injury_type_classifications and
         rehabilitation_program_types) into readable comma-separated strings.

    Returns
    -------
    pd.DataFrame
        Cleaned copy of the input DataFrame.
    """
    df = df.copy()

    # 1. Empty strings -> NaN
    df.replace(r'^\s*$', np.nan, regex=True, inplace=True)

    # 2. Remove duplicate rows
    before = len(df)
    df = df.drop_duplicates()
    after = len(df)
    print(f"Removed {before - after} duplicate rows.")

    # 3. Convert set columns into comma separated strings
    set_like_cols = ['injury_type_classifications', 'rehabilitation_program_types']

    def tidy_set_column(series: pd.Series) -> pd.Series:
        def convert_value(x):
            if pd.isna(x):
                return np.nan
            # if it's a string representing a set, try to parse it
            if isinstance(x, str):
                try:
                    parsed = ast.literal_eval(x)
                    x = parsed
                except (ValueError, SyntaxError):
                    return x.strip()
            # if it's a set/list/tuple, sort and join as a string
            if isinstance(x, (set, list, tuple)):
                items = sorted(str(item) for item in x)
                return ", ".join(items)
            # fallback: just string form
            return str(x).strip()
        return series.apply(convert_value)

    for col in set_like_cols:
        if col in df.columns:
            df[col] = tidy_set_column(df[col])

    # 4. Normalise categorical columns (strip + lowercase)
    #    for all *other* object columns
    object_cols = df.select_dtypes(include='object').columns.tolist()
    cols_to_standardise = [
        col for col in object_cols
        if col not in set_like_cols
    ]

    for col in cols_to_standardise:
        df[col] = df[col].astype(str).str.strip().str.lower()

    # 5. Convert numeric columns (currently objects) to numeric dtype
    numeric_cols = [
        'claim_value_category',
        'injury_duration_days',
        'work_absence_duration_days',
        'defendant_title_code',
        'claimant_age_at_incident',
        'rental_vehicle_expense',
        'home_care_services_cost',
        'employment_disadvantage_compensation',
        'career_satisfaction_loss_compensation',
        'asset_utility_loss_compensation',
        'medical_treatment_costs',
        'general_damages',
        'insurance_deductible_amount',
        'total_claim_amount',
        'claimant_current_age_years',
        'medical_attention_delay_days',
    ]

    for col in numeric_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')

    # 6. Basic checks

    # columns that should not be negative
    non_negative_cols = [
        'injury_duration_days',
        'work_absence_duration_days',
        'rental_vehicle_expense',
        'home_care_services_cost',
        'employment_disadvantage_compensation',
        'career_satisfaction_loss_compensation',
        'asset_utility_loss_compensation',
        'medical_treatment_costs',
        'general_damages',
        'insurance_deductible_amount',
        'total_claim_amount',
        'medical_attention_delay_days',
    ]

    for col in non_negative_cols:
        if col in df.columns:
            n_neg = (df[col] < 0).sum()
            if n_neg > 0:
                print(f"{col}: setting {n_neg} negative values to NaN.")
                df.loc[df[col] < 0, col] = np.nan

    # ages should be between 0 and 110 (check)
    for col in ['claimant_age_at_incident', 'claimant_current_age_years']:
        if col in df.columns:
            n_bad = ((df[col] <= 0) | (df[col] > 110)).sum()
            if n_bad > 0:
                print(f"{col}: setting {n_bad} unrealistic ages to NaN.")
                df.loc[(df[col] <= 0) | (df[col] > 110), col] = np.nan

    return df

#This is for the code to save the final 'cleaned' Version, if you already have this comment eveything back !
# 1. Load original file
df = pd.read_csv("motor_insurance_recovery.csv")

# 2. Clean it
df_clean = clean_motor_insurance_data(df)
df_export = df_clean.fillna("Blank")

# 3. Inspect result
print("\n=== Cleaned DataFrame info ===")
df_clean.info()

# 4.Save cleaned file – uncomment this !!
df_export.to_csv("motor_insurance_recovery_cleaned.csv", index=False)

