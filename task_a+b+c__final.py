# -*- coding: utf-8 -*-
"""TASK_A+B+C__Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eNTkmtRSSDmHk6JYMfT1nWshLg3PN8XZ
"""

from __future__ import annotations
import ast
import numpy as np
import pandas as pd
#ast is used to safely convert set-like strings (e.g. "{'Head', 'Soft Tissue'}") into real Python objects.

# -----------------------------
# Helper: Logging
# -----------------------------
def log(msg: str) -> None:
    """
    Simple logger for Task A steps.
    Prefixes messages with [TaskA] for easier debugging.
    """
    print(f"[TaskA] {msg}")
#This function ensures all diagnostic messages have the prefix [TaskA], making it easier to trace each step of the cleaning pipeline.
#It improves debugging and makes console outputs professional and readable.

# -----------------------------
# Helper: Smart CSV Reader
# -----------------------------
def smart_read_csv(path: str) -> pd.DataFrame:

    seps = [None, ",", ";", "\t", "|"]
    for sep in seps:
        try:
            df = pd.read_csv(path, sep=sep, engine="python")
            if df.shape[1] >= 20:
                log(f"Loaded successfully with sep={repr(sep)} shape={df.shape}")
                return df
        except Exception:
            continue
    raise ValueError("Could not detect valid CSV delimiter!")
#Some CSVs use comma, some semicolon, some tab.
#This function tries multiple delimiters until the file loads with a reasonable number of columns (>=20).
#This method makes the program robust and ensures it can handle different CSV formats gracefully.

# -----------------------------
# Load the dataset
# -----------------------------
file_path = "motor_insurance_recovery.csv"  # adjust if needed
df = smart_read_csv(file_path)
log(f"Initial shape: {df.shape}")

# -----------------------------
# Basic Exploration
# -----------------------------
log("Basic dataset info:")
print("\n[TaskA] Dataset shape (rows, columns):", df.shape, "\n")

log("DataFrame.info():")
df.info()
print()

log("Summary statistics for numeric columns:")
print(df.describe())
print()

log("Missing values per column (before cleaning):")
print(df.isna().sum().sort_values(ascending=False))
print()

# -----------------------------
# Text Cleaning: strip + lower
# -----------------------------
log("Converting all object columns to stripped + lowercase strings.")
obj_cols = df.select_dtypes(include=["object"]).columns
for c in obj_cols:
    df[c] = df[c].astype(str).str.strip().str.lower()

# -----------------------------
# Standardise car_damage_severity values
# -----------------------------
if "car_damage_severity" in df.columns:
    sev_map = {
        "none": "none",
        "no": "none",
        "": np.nan,
        "nan": np.nan,
        "minor": "minor",
        "med": "moderate",
        "moderate": "moderate",
        "severe": "severe",
        "sev": "severe",
        "total_loss": "total_loss"
    }
    df["car_damage_severity"] = df["car_damage_severity"].replace(sev_map)
    log("Standardised car_damage_severity values.")

# -----------------------------
# Helper: Safe parse for set-like text fields
# -----------------------------
def safe_parse_to_sorted_text(cell: object) -> str:
    """
    Convert set-like text such as:
        "{'Head Injury', 'Soft Tissue Injury'}"
    into a clean, sorted, lowercase comma-separated string like:
        "head injury, soft tissue injury"

    Returns "none" for empty/invalid values.
    """
    if pd.isna(cell):
        return "none"
    txt = str(cell).strip()
    if not txt or txt in {"nan", "none", "{}", "set()"}:
        return "none"
    try:
        obj = ast.literal_eval(txt)
        if isinstance(obj, (set, list, tuple)):
            vals = sorted({str(x).strip().lower() for x in obj if str(x).strip()})
            return ", ".join(vals) if vals else "none"
        return str(obj).strip().lower() or "none"
    except Exception:
        inner = txt.strip("{} ")
        items = [x.strip().strip("'\"").lower() for x in inner.split(",") if x.strip()]
        return ", ".join(sorted(set(items))) if items else "none"


# -----------------------------
# Apply parsing to set-like fields
# -----------------------------
for col in ["injury_type_classifications", "rehabilitation_program_types"]:
    if col in df.columns:
        df[col] = df[col].apply(safe_parse_to_sorted_text)
        log(f"Parsed set-like column: {col}")

# -----------------------------
# Detect binary-like columns
# -----------------------------
def is_binary_like(s: pd.Series) -> bool:
    """
    Check if a series only contains {0,1} or {True,False} (ignoring NaN).
    """
    vals = set(s.dropna().unique().tolist())
    allowed = {0, 1, 0.0, 1.0, True, False}
    return len(vals) > 0 and vals.issubset(allowed)


binary_cols = [c for c in df.columns if is_binary_like(df[c])]
if binary_cols:
    df[binary_cols] = df[binary_cols].astype("Int64")
    log(f"Converted binary-like columns to Int64: {binary_cols}")
else:
    log("No binary-like columns detected.")


# -----------------------------
# Clamp negative values to zero for non-negative columns
# -----------------------------
nonneg_cols = [
    "injury_duration_days",
    "work_absence_duration_days",
    "rental_vehicle_expense",
    "home_care_services_cost",
    "employment_disadvantage_compensation",
    "career_satisfaction_loss_compensation",
    "asset_utility_loss_compensation",
    "medical_treatment_costs",
    "general_damages",
    "insurance_deductible_amount",
    "total_claim_amount",
    "claimant_age_at_incident",
    "claimant_current_age_years",
    "medical_attention_delay_days"
]

for c in nonneg_cols:
    if c in df.columns:
        neg_count = (df[c] < 0).sum()
        if neg_count > 0:
            df.loc[df[c] < 0, c] = 0
            log(f"Clipped {neg_count} negative values in {c} to zero.")


# -----------------------------
# Admin / free-text columns → 'not_provided'
# -----------------------------
admin_cols = [
    "claims_resolution_exit_notes",
    "claims_management_exit_notes",
    "claim_rejection_code",
    "liability_denial_reasons",
    "claims_resolution_closure_code"
]

for c in admin_cols:
    if c in df.columns:
        df[c] = df[c].replace({"": np.nan, "nan": np.nan}).fillna("not_provided")
        #"not_provided" simply means the information wasn’t given or doesn’t apply, rather than it being a true missing value.

log("Filled admin/free-text columns with 'not_provided' where missing.")

# -----------------------------
# Numeric imputation (excluding binary columns)
# -----------------------------
num_cols = df.select_dtypes(include=[np.number, "Int64"]).columns
num_cols = [c for c in num_cols if c not in binary_cols]  # binary kolonları hariç tut

for c in num_cols:
    missing = df[c].isna().sum()
    if missing == 0:
        continue
    skew_val = df[c].dropna().skew()
    if skew_val > 1:
        fill_val = df[c].median()
        method = "median"
    else:
        fill_val = df[c].mean()
        method = "mean"
    df[c] = df[c].fillna(fill_val)
    log(f"Filled {missing} missing values in numeric column '{c}' using {method} ({fill_val:.2f}).")


# -----------------------------
# Categorical imputation (mode or 'not_provided')
# -----------------------------
cat_cols = df.select_dtypes(include=["object"]).columns
for c in cat_cols:
    missing = df[c].isna().sum()
    if missing == 0:
        continue
    mode = df[c].mode(dropna=True)
    if not mode.empty:
        df[c] = df[c].fillna(mode[0])
        log(f"Filled {missing} missing values in categorical column '{c}' with mode '{mode[0]}'.")
    else:
        df[c] = df[c].fillna("not_provided")
        log(f"Filled {missing} missing values in categorical column '{c}' with 'not_provided'.")

# -----------------------------
# Derived feature: num_injuries
# -----------------------------
if "injury_type_classifications" in df.columns:
    def count_injuries(x: str) -> int:
        if x == "none":
            return 0
        return len([i for i in x.split(",") if i.strip()])

    df["num_injuries"] = df["injury_type_classifications"].apply(count_injuries).astype("Int64")
    log("Created 'num_injuries' feature from 'injury_type_classifications'.")
#This code creates a new feature called num_injuries, which counts how many separate injury types each claim contains.
#If the injury field is "none", the count is 0.
#Otherwise, it splits the injury list by commas and counts the number of valid injury labels.

# -----------------------------
# Derived feature: car_damage_severity_num
# -----------------------------
if "car_damage_severity" in df.columns:
    sev_numeric_map = {
        "none": 0,
        "minor": 1,
        "moderate": 2,
        "severe": 3,
        "total_loss": 3  # Because “total_loss” represents the highest possible
        #level of vehicle damage, similar in severity to “severe”, and both reflect
        #situations where the car is effectively beyond economic repair.
    }
    df["car_damage_severity_num"] = df["car_damage_severity"].map(sev_numeric_map).astype("Int64")
    log("Created numeric 'car_damage_severity_num' feature.")

# -----------------------------
# Duplicate checks
# -----------------------------
log("Duplicate checks:")
dup_rows = df.duplicated().sum()
log(f"Number of fully duplicated rows: {dup_rows}")

if "client_id" in df.columns:
    dup_clients = df["client_id"].duplicated().sum()
    log(f"Number of duplicated client_id values: {dup_clients}")


# -----------------------------
# IQR-based outlier counts
# -----------------------------
def iqr_outlier_count(s: pd.Series) -> int:
    s = s.dropna()
    if s.empty:
        return 0
    q1, q3 = s.quantile([0.25, 0.75])
    iqr = q3 - q1
    lower, upper = q1 - 1.5 * iqr, q3 + 1.5 * iqr
    return ((s < lower) | (s > upper)).sum()


numeric_for_outliers = df.select_dtypes(include=[np.number, "Int64"]).columns
outliers = {col: iqr_outlier_count(df[col]) for col in numeric_for_outliers}

outlier_df = (
    pd.DataFrame(list(outliers.items()), columns=["column", "outlier_count"])
      .sort_values("outlier_count", ascending=False)
)

print("\nTop-10 numeric columns by outlier count (IQR method):")
print(outlier_df.head(10))


# -----------------------------
# Final summary
# -----------------------------
log(f"Final shape after cleaning: {df.shape}")
total_missing = df.isna().sum().sum()
log(f"Total remaining missing values in dataframe: {total_missing}")

log("Missing values per column (after cleaning):")
print(df.isna().sum().sort_values(ascending=False))

import matplotlib.pyplot as plt-

# 1) Missing table – after cleaning
missing_counts = df.isna().sum()
missing_pct = (missing_counts / len(df)) * 100

missing_table = (
    pd.DataFrame({
        "missing_count": missing_counts,
        "missing_percent": missing_pct.round(2)
    })
    .sort_values("missing_count", ascending=False)
)

print("\n[TaskA] Missing values per column AFTER cleaning:")
print(missing_table)

# Grafik: missing bar chart
non_zero_missing = missing_table[missing_table["missing_count"] > 0]

plt.figure(figsize=(10, 4))
if not non_zero_missing.empty:
    non_zero_missing["missing_count"].plot(kind="bar")
    plt.title("Columns with Remaining Missing Values After Cleaning")
    plt.ylabel("Number of Missing Values")
    plt.tight_layout()
    plt.show()
else:
    print("\n[TaskA] No missing values remain in any column after cleaning.")

cat_summary_cols = [
    "car_damage_severity",
    "hospital_visit_required",
    "hospital_admission_required",
    "rehabilitation_recommended",
    "rehabilitation_completed",
    "emergency_services_attended",
]

for c in cat_summary_cols:
    if c in df.columns:
        print(f"\n[TaskA] Frequency table for {c}:")
        vc = df[c].value_counts(dropna=False)
        pct = (vc / len(df) * 100).round(2)
        freq_df = pd.DataFrame({"count": vc, "percent": pct})
        print(freq_df)

        # Bar plot
        plt.figure(figsize=(6, 4))
        vc.plot(kind="bar")
        plt.title(f"{c} – value counts")
        plt.ylabel("Count")
        plt.tight_layout()
        plt.show()

numeric_to_plot = [
    "total_claim_amount",
    "general_damages",
    "medical_treatment_costs",
    "injury_duration_days",
]

for col in numeric_to_plot:
    if col in df.columns:
        print(f"\n[TaskA] Summary for {col}:")
        print(df[col].describe())

        # Histogram
        plt.figure(figsize=(6, 4))
        df[col].hist(bins=30)
        plt.title(f"Distribution of {col}")
        plt.xlabel(col)
        plt.ylabel("Frequency")
        plt.tight_layout()
        plt.show()

        # Boxplot
        plt.figure(figsize=(4, 5))
        df.boxplot(column=col)
        plt.title(f"Boxplot of {col}")
        plt.tight_layout()
        plt.show()

summary_overview = {
    "rows": len(df),
    "columns": df.shape[1],
    "total_missing_values": int(df.isna().sum().sum()),
    "num_numeric_columns": df.select_dtypes(include=[np.number, "Int64"]).shape[1],
    "num_categorical_columns": df.select_dtypes(include=["object"]).shape[1],
}

print("\n[TaskA] Final cleaned dataset overview:")
for k, v in summary_overview.items():
    print(f"  - {k}: {v}")

pd.set_option("display.max_columns", None)
pd.set_option("display.width", 2000)

df.head(30)

"""# **TASK B**"""

# ============================================================
# TASK B: Injury & Rehabilitation Analysis for Motor Insurance
# ============================================================
import matplotlib.pyplot as plt

pd.set_option("display.max_columns", None)
pd.set_option("display.width", 1800)

# ============================================================
# 2.1 INJURY PATTERN ANALYSIS
# ============================================================
def prepare_injury_lists(df: pd.DataFrame) -> pd.DataFrame:
    """
    Ensure injury_type_classifications is available as a list
    in a helper column 'injury_list' for exploded analysis.
    """
    df = df.copy()
    df["injury_list"] = df["injury_type_classifications"].apply(
        lambda x: [i.strip() for i in str(x).split(",") if i.strip()]
        if str(x).lower() != "none" else []
    )
    return df


def analyze_injury_patterns(df: pd.DataFrame) -> dict:
    """
    Perform injury pattern analysis:
    - Frequency of individual injury types
    - Mean total_claim_amount and injury_duration_days per injury type
    - Basic plots for distribution and cost impact

    Returns a dictionary with key summary tables for later use.
    """
    log("[TaskB] Starting Injury Pattern Analysis...")
    df_local = prepare_injury_lists(df)

    # All injuries exploded
    injuries_exploded = df_local.explode("injury_list")
    injuries_exploded = injuries_exploded[injuries_exploded["injury_list"].notna()]

    # Frequency table
    freq = (
        injuries_exploded["injury_list"]
        .value_counts()
        .to_frame("count")
    )
    freq["percent_of_claims"] = (freq["count"] / len(df_local) * 100).round(2)

    print("\n[TaskB] Injury type frequency (top 20):")
    print(freq.head(20))

    # Outcomes by injury type
    outcome = (
        injuries_exploded
        .groupby("injury_list")
        .agg(
            mean_total_claim=("total_claim_amount", "mean"),
            median_total_claim=("total_claim_amount", "median"),
            mean_injury_duration=("injury_duration_days", "mean"),
            median_injury_duration=("injury_duration_days", "median"),
            num_claims=("client_id", "nunique"),
        )
        .sort_values("mean_total_claim", ascending=False)
        .round(2)
    )

    print("\n[TaskB] Injury type vs claim outcomes (top 15 by mean_total_claim):")
    print(outcome.head(15))

    # Plot 1: Frequency of top injuries
    top_n = 10
    plt.figure(figsize=(8, 4))
    freq.head(top_n)["count"].plot(kind="bar")
    plt.title(f"Top {top_n} Injury Types by Frequency")
    plt.ylabel("Number of Claims")
    plt.xticks(rotation=45, ha="right")
    plt.tight_layout()
    plt.show()

    # Plot 2: Mean claim amount by top injuries (based on cost)
    plt.figure(figsize=(8, 4))
    outcome.head(top_n)["mean_total_claim"].plot(kind="bar")
    plt.title(f"Top {top_n} Injury Types by Mean Total Claim Amount")
    plt.ylabel("Mean total_claim_amount")
    plt.xticks(rotation=45, ha="right")
    plt.tight_layout()
    plt.show()

    # Plot 3: Mean injury duration by top injuries (based on duration)
    outcome_by_duration = outcome.sort_values("mean_injury_duration", ascending=False)

    plt.figure(figsize=(8, 4))
    outcome_by_duration.head(top_n)["mean_injury_duration"].plot(kind="bar")
    plt.title(f"Top {top_n} Injury Types by Mean Injury Duration (days)")
    plt.ylabel("Mean injury_duration_days")
    plt.xticks(rotation=45, ha="right")
    plt.tight_layout()
    plt.show()

    log("[TaskB] Injury Pattern Analysis completed.")
    return {
        "injury_frequency": freq,
        "injury_outcomes": outcome,
        "injuries_exploded": injuries_exploded,
    }


injury_results = analyze_injury_patterns(df)

# ============================================================
# 2.2 REHABILITATION PROGRAM ANALYSIS
# ============================================================

def prepare_rehab_lists(df: pd.DataFrame) -> pd.DataFrame:
    """
    Ensure rehabilitation_program_types is represented as a list
    in 'rehab_prog_list' for exploded analysis.
    """
    df = df.copy()
    df["rehab_prog_list"] = df["rehabilitation_program_types"].apply(
        lambda x: [i.strip() for i in str(x).split(",") if i.strip()]
        if str(x).lower() != "none" else []
    )
    return df


def analyze_rehabilitation(df: pd.DataFrame) -> dict:
    """
    Analyse rehabilitation recommendation and completion patterns,
    their relationship with outcomes and program types.

    Returns summary tables for reporting.
    """
    log("[TaskB] Starting Rehabilitation Program Analysis...")
    df_local = prepare_rehab_lists(df)

    # Overall recommendation / completion rates
    rec_rate = df_local["rehabilitation_recommended"].mean()
    comp_rate = df_local["rehabilitation_completed"].mean()

    print("\n[TaskB] Rehabilitation headline rates:")
    print(f"  - Recommendation rate: {rec_rate:.3f}")
    print(f"  - Completion rate    : {comp_rate:.3f}")

    # Rehab completion vs outcomes
    rehab_impact = (
        df_local.groupby("rehabilitation_completed")
        .agg(
            mean_total_claim=("total_claim_amount", "mean"),
            median_total_claim=("total_claim_amount", "median"),
            mean_injury_duration=("injury_duration_days", "mean"),
            mean_work_absence=("work_absence_duration_days", "mean"),
            count=("client_id", "nunique"),
        )
        .round(2)
    )

    print("\n[TaskB] Rehabilitation completion vs outcomes:")
    print(rehab_impact)

    # Bar plot: completion vs mean duration / claim
    plt.figure(figsize=(6, 4))
    rehab_impact["mean_injury_duration"].plot(kind="bar")
    plt.title("Mean Injury Duration by Rehab Completion")
    plt.ylabel("Mean injury_duration_days")
    plt.xticks(rotation=0)
    plt.tight_layout()
    plt.show()

    plt.figure(figsize=(6, 4))
    rehab_impact["mean_total_claim"].plot(kind="bar")
    plt.title("Mean Total Claim Amount by Rehab Completion")
    plt.ylabel("Mean total_claim_amount")
    plt.xticks(rotation=0)
    plt.tight_layout()
    plt.show()

    # Program-level analysis
    rehab_exploded = df_local.explode("rehab_prog_list")
    rehab_exploded = rehab_exploded[rehab_exploded["rehab_prog_list"].notna()]

    prog_summary = (
        rehab_exploded
        .groupby("rehab_prog_list")
        .agg(
            num_claims=("client_id", "nunique"),
            mean_injury_duration=("injury_duration_days", "mean"),
            mean_total_claim=("total_claim_amount", "mean"),
            completion_rate=("rehabilitation_completed", "mean"),
        )
        .sort_values("num_claims", ascending=False)
        .round(2)
    )

    print("\n[TaskB] Rehabilitation program type summary (top 15):")
    print(prog_summary.head(15))

    # Plot: top rehab programs by volume
    plt.figure(figsize=(8, 4))
    prog_summary.head(10)["num_claims"].plot(kind="bar")
    plt.title("Top Rehab Programs by Number of Claims")
    plt.ylabel("Number of Claims")
    plt.xticks(rotation=45, ha="right")
    plt.tight_layout()
    plt.show()

    log("[TaskB] Rehabilitation Program Analysis completed.")
    return {
        "rehab_impact": rehab_impact,
        "rehab_program_summary": prog_summary,
        "rehab_exploded": rehab_exploded,
    }


rehab_results = analyze_rehabilitation(df)

# ============================================================
# 2.3 INJURY SEVERITY SCORING (AIS / ISS + PROXY INDEX)
# ============================================================

# (1) Injury → AIS severity mapping
# This mapping is inspired by the general AIS (Abbreviated Injury Scale) logic
# but adapted to the context of motor insurance data.
# 1 = minor, 2 = moderate, 3 = serious, 4+ = more critical
ais_severity_map = {
    "soft tissue injury": 1,
    "whiplash": 1,                    # typical whiplash is usually mild / moderate
    "burns and scalds": 2,
    "bone injury": 2,
    "facial injury": 2,
    "miscellaneous": 1,
    "psychological issues": 2,        # non-anatomical but we reflect its impact with AIS 2
    "internal injury": 3,
    "spinal injury": 3,
    "head injury": 3,
    # Any unknown injury type will default to AIS = 1 (minor).
}

# (2) Injury → body region mapping (ISS regions)
# We use the standard ISS body regions:
#   1. Head/Neck
#   2. Face
#   3. Chest
#   4. Abdomen / Pelvic contents
#   5. Extremities / Pelvic girdle
#   6. External
# We encode them as simple string labels.
body_region_map = {
    "head injury": "head_neck",
    "whiplash": "head_neck",
    "soft tissue injury": "external",
    "burns and scalds": "external",
    "facial injury": "face",
    "internal injury": "abdomen_pelvis",
    "spinal injury": "extremities_pelvic",
    "bone injury": "extremities_pelvic",
    "psychological issues": "head_neck",  # psychological impact is often associated with head/neck trauma
    "miscellaneous": "external",
}


def parse_injury_labels(injury_str: str) -> list[str]:
    """
    Convert a cleaned injury_type_classifications string into a list of labels.

    Example
    -------
    'bone injury, burns and scalds, head injury, whiplash'
        → ['bone injury', 'burns and scalds', 'head injury', 'whiplash']

    If the value is 'none' or not a valid string, an empty list is returned.
    """
    if not isinstance(injury_str, str):
        return []
    if injury_str.strip().lower() == "none":
        return []
    return [x.strip() for x in injury_str.split(",") if x.strip()]


def injury_to_ais_and_region(label: str) -> tuple[int, str]:
    """
    Map a single injury label to an (AIS_score, body_region) pair.

    Unknown labels are treated as (1, 'external'), i.e. a minor,
    external-type injury. This makes the model robust to unexpected
    text values while keeping them conservatively minor.
    """
    label_lower = label.strip().lower()
    ais_val = ais_severity_map.get(label_lower, 1)
    region = body_region_map.get(label_lower, "external")
    return ais_val, region


def compute_ais_profile_for_claim(injury_str: str) -> dict:
    """
    Compute the AIS profile for a single claim.

    Parameters
    ----------
    injury_str : str
        Cleaned injury_type_classifications string.

    Returns
    -------
    dict with:
        - 'ais_scores': list of AIS scores for all injuries in the claim.
        - 'region_max_ais': dict mapping body_region → max AIS observed in that region.

    This representation is then used to compute an ISS-like score per claim.
    """
    labels = parse_injury_labels(injury_str)
    if not labels:
        return {
            "ais_scores": [],
            "region_max_ais": {},
        }

    region_max: dict[str, int] = {}
    scores: list[int] = []

    for lab in labels:
        ais_val, region = injury_to_ais_and_region(lab)
        scores.append(ais_val)
        current = region_max.get(region, 0)
        if ais_val > current:
            region_max[region] = ais_val

    return {
        "ais_scores": scores,
        "region_max_ais": region_max,
    }


def compute_iss_from_regions(region_max_ais: dict) -> int:
    """
    Compute an ISS-like score from the maximum AIS per body region.

    Logic
    -----
    - Take the maximum AIS in each body region.
    - Select the three highest regional AIS scores.
    - ISS-like score = sum of squares of these three AIS scores.
    - If any AIS = 6 (not used here but kept for completeness),
      ISS is set to 75 by convention.

    Returns
    -------
    int
        ISS-like score in the range [0, 75].
    """
    if not region_max_ais:
        return 0

    scores = list(region_max_ais.values())
    if any(s == 6 for s in scores):
        return 75

    top3 = sorted(scores, reverse=True)[:3]
    iss = sum(s ** 2 for s in top3)
    return int(iss)


def compute_severity_index(row: pd.Series) -> float:
    """
    Compute a composite severity index for a claim by combining:

    - ISS-like anatomical score (iss_like)
    - injury_duration_days
    - car_damage_severity_num (proxy for impact severity)
    - general_damages (pain, suffering, loss of amenity)
    - rehabilitation_recommended + rehabilitation_completed (rehab intensity)
    - presence of non-visible injuries:
        * whiplash
        * psychological issues

    The coefficients are chosen to:
    - Keep the scale interpretable.
    - Reflect that longer recovery, higher vehicle damage,
      higher general damages and more intensive rehab
      are indicators of higher overall severity.

    Returns
    -------
    float
        Composite severity index (continuous score).
    """

    iss = row["iss_like"]
    duration = row["injury_duration_days"]
    car_sev = row.get("car_damage_severity_num", 0)
    gen_dmg = row["general_damages"]
    rehab_intensity = row["rehabilitation_recommended"] + row["rehabilitation_completed"]

    injuries = str(row["injury_type_classifications"]).lower()
    has_whiplash = "whiplash" in injuries
    has_psych = "psychological issues" in injuries

    # Base: ISS-like anatomical severity (0–75)
    score = float(iss)

    # Proxy contributions based on the assignment guidance:
    # - Longer injury duration = higher clinical and economic severity
    score += 0.05 * duration          # ~ +1 point per 20 days

    # - Higher car damage severity suggests a more forceful collision
    score += 1.0 * car_sev            # car_damage_severity_num ∈ {0,1,2,3}

    # - General damages approximate pain and suffering
    score += 0.0001 * gen_dmg         # ~ +1 point per £10,000

    # - Rehab intensity (recommended + completed) reflects complexity
    score += 0.5 * rehab_intensity

    # - Non-visible injuries (whiplash, psychological issues):
    #   we add discrete increments to capture their impact.
    if has_whiplash and duration >= 90:
        # Chronic whiplash: long-lasting symptoms
        score += 2.0
    elif has_whiplash:
        score += 1.0

    if has_psych:
        # Psychological injuries can significantly reduce quality of life
        score += 1.5

    return round(score, 2)


def classify_iss_band(iss_value: int) -> str:
    """
    Classify the ISS-like score into clinically-inspired severity bands.

    Bands
    -----
    - 0     : 'no_injury'
    - 1–8   : 'minor'
    - 9–15  : 'moderate'
    - 16–24 : 'serious'
    - 25+   : 'severe_critical'

    Returns
    -------
    str
        Severity category label.
    """
    if iss_value == 0:
        return "no_injury"
    elif iss_value <= 8:
        return "minor"
    elif iss_value <= 15:
        return "moderate"
    elif iss_value <= 24:
        return "serious"
    else:
        return "severe_critical"


def apply_injury_severity_scoring(df: pd.DataFrame) -> pd.DataFrame:
    """
    Apply the AIS / ISS-inspired injury severity system to all claims
    in the given DataFrame and compute:

    - ais_scores_list   : list of AIS scores per claim
    - region_max_ais    : dict of region → max AIS per claim
    - iss_like          : ISS-like score (sum of squares of top 3 regions)
    - iss_band          : categorical severity band
    - severity_index    : composite proxy-based severity index

    Parameters
    ----------
    df : pd.DataFrame
        Cleaned claims dataset (output from Task A).

    Returns
    -------
    pd.DataFrame
        Copy of the input DataFrame with the additional severity fields.
    """
    log("[TaskB] Applying AIS / ISS severity scoring to all claims...")
    df = df.copy()

    ais_profiles = df["injury_type_classifications"].apply(compute_ais_profile_for_claim)
    df["ais_scores_list"] = ais_profiles.apply(lambda x: x["ais_scores"])
    df["region_max_ais"] = ais_profiles.apply(lambda x: x["region_max_ais"])

    df["iss_like"] = df["region_max_ais"].apply(compute_iss_from_regions).astype(int)

    # ISS band (minor / moderate / serious / severe_critical)
    df["iss_band"] = df["iss_like"].apply(classify_iss_band)

    # Composite severity index
    df["severity_index"] = df.apply(compute_severity_index, axis=1)

    log("[TaskB] AIS / ISS scoring completed.")
    return df


# Apply the scoring to the main cleaned DataFrame (df from Task A)
df_b = apply_injury_severity_scoring(df)  # df_b: Task B enriched dataframe

print("\n[TaskB] ISS-like score summary:")
print(df_b["iss_like"].describe())

print("\n[TaskB] Severity index summary:")
print(df_b["severity_index"].describe())

print("\n[TaskB] ISS band counts:")
print(df_b["iss_band"].value_counts())

# ============================================================
# 2.3(b) DATA-DRIVEN REFINEMENT OF SEVERITY INDEX
# ============================================================

def fit_data_driven_severity_index(df: pd.DataFrame) -> pd.DataFrame:
    """
    Construct a data-driven severity index by fitting a simple
    linear regression model from key severity-related features to
    total_claim_amount, and then using the learned weights to build
    a calibrated severity score.

    Idea
    ----
    - We keep the AIS/ISS logic as the structural backbone.
    - We then let the data inform how much each proxy should
      contribute to an overall severity score.

    Steps
    -----
    1. Select severity-related predictors:
         - iss_like
         - injury_duration_days
         - car_damage_severity_num
         - general_damages
         - rehabilitation_recommended
         - rehabilitation_completed
    2. Standardise each predictor (mean 0, std 1) so that
       regression coefficients are comparable.
    3. Fit a simple linear model:
         total_claim_amount ≈ β0 + β1 x1 + ... + βk xk
    4. Use the linear predictor (without the intercept) as a
       raw severity score and rescale it to [0, 100] for
       interpretability.

    Returns
    -------
    pd.DataFrame
        Input DataFrame with an additional column:
            - severity_index_data_driven
    """
    log("[TaskB] Fitting data-driven severity index...")

    df = df.copy()

    # 1) Select features (make sure all exist)
    feature_cols = [
        "iss_like",
        "injury_duration_days",
        "car_damage_severity_num",
        "general_damages",
        "rehabilitation_recommended",
        "rehabilitation_completed",
    ]

    for col in feature_cols:
        if col not in df.columns:
            raise KeyError(f"Required feature column '{col}' is missing from DataFrame.")

    # Filter to rows with no missing values in chosen features and target
    model_df = df.dropna(subset=feature_cols + ["total_claim_amount"]).copy()

    if model_df.empty:
        raise ValueError("No valid rows available to fit the data-driven severity model.")

    X = model_df[feature_cols].to_numpy(dtype=float)
    y = model_df["total_claim_amount"].to_numpy(dtype=float)

    # 2) Standardise predictors (z-scores)
    means = X.mean(axis=0)
    stds = X.std(axis=0, ddof=0)
    stds[stds == 0] = 1.0  # avoid division by zero

    X_std = (X - means) / stds

    # 3) Fit linear regression using normal equations
    #    β = (X^T X)^(-1) X^T y
    XtX = X_std.T @ X_std
    Xty = X_std.T @ y
    betas = np.linalg.solve(XtX, Xty)  # shape (k,)

    # 4) Compute raw severity scores (no intercept; just weighted sum of z-scores)
    raw_scores = X_std @ betas  # shape (n,)

    # Rescale raw_scores to [0, 100] for interpretability
    s_min, s_max = raw_scores.min(), raw_scores.max()
    if s_max == s_min:
        # All scores equal → degenerate case, just use zeros
        scaled_scores = np.zeros_like(raw_scores)
    else:
        scaled_scores = 100 * (raw_scores - s_min) / (s_max - s_min)

    # Attach back to the full DataFrame
    model_df["severity_index_data_driven"] = scaled_scores

    # Merge the new column into the original df (align on index)
    df["severity_index_data_driven"] = model_df["severity_index_data_driven"]

    # Quick diagnostics: correlations with key outcomes
    corr_df = df[[
        "iss_like",
        "severity_index",
        "severity_index_data_driven",
        "total_claim_amount",
        "injury_duration_days",
    ]].corr()

    print("\n[TaskB] Correlation matrix (including data-driven severity index):")
    print(corr_df)

    log("[TaskB] Data-driven severity index fitted and added as 'severity_index_data_driven'.")
    return df


# Apply the data-driven refinement on top of df_b (which already has iss_like & severity_index)
df_b = fit_data_driven_severity_index(df_b)

print("\n[TaskB] Data-driven severity index summary:")
print(df_b["severity_index_data_driven"].describe())

# ===============================================
# TASK B – FULL TEST PIPELINE
# ===============================================

task_b_test_data = {
    "client_id": [1.0, 6.0, 8.0],
    "claim_value_category": [1.0, 2.0, 2.0],
    "injury_duration_days": [33.0, 44.0, 42.0],
    "hospital_visit_required": [1.0, 1.0, 1.0],
    "medical_care_sought": [1.0, 1.0, 1.0],
    "hospital_admission_required": [0.0, 0.0, 1.0],
    "currently_unemployed_due_to_injury": [0.0, 1.0, 0.0],
    "work_absence_duration_days": [2.0, 12.0, 0.0],
    "work_absence_required": [0.0, 1.0, 0.0],
    "rehabilitation_recommended": [0.0, 1.0, 1.0],
    "rehabilitation_completed": [0.0, 1.0, 1.0],
    "minor_claimant_at_initial_assessment": [0.0, 0.0, 0.0],
    "defendant_title_code": [0.0, 1.0, 0.0],
    "liability_admission_status": ["A", "A", "A"],
    "liability_type": ["motor", "motor", "motor"],
    "claimant_age_at_incident": [55.0, 31.0, 47.0],
    "liability_denial_reasons": ["", "", ""],
    "claim_rejection_code": ["", "", ""],
    "claims_management_exit_notes": ["", "", ""],
    "claims_resolution_exit_notes": ["", "", ""],
    "claims_resolution_closure_code": ["", "", ""],
    "rental_vehicle_expense": [0.0, 0.0, 0.0],
    "home_care_services_cost": [0.0, 0.0, 0.0],
    "employment_disadvantage_compensation": [0.0, 0.0, 0.0],
    "career_satisfaction_loss_compensation": [0.0, 0.0, 0.0],
    "asset_utility_loss_compensation": [0.0, 0.0, 0.0],
    "medical_treatment_costs": [125.32, 250.73, 500.12],
    "general_damages": [6056.74, 14563.83, 24008.63],
    "insurance_deductible_amount": [50.0, 100.0, 0.0],
    "car_damage_severity": ["none", "minor", "severe"],
    "total_claim_amount": [6232.06, 14914.56, 24508.75],
    "injury_type_classifications": [
        "{'Internal Injury', 'Psychological Issues'}",
        "{'Facial Injury', 'Psychological Issues', 'Soft Tissue Injury'}",
        "{'Head Injury', 'Miscellaneous', 'Soft Tissue Injury'}",
    ],
    "rehabilitation_program_types": [
        "{'Unknown'}",
        "{'exercises', 'pain relief'}",
        "{'miscellaneous', 'scans and imaging', 'surgery'}",
    ],
    "claimant_current_age_years": [55.0, 31.0, 47.0],
    "medical_attention_delay_days": [0.0, 0.0, 0.0],
    "emergency_services_attended": ["none", "ambulance", "ambulance_and_police"],
}

df_test = pd.DataFrame(task_b_test_data)

print("[TEST] RAW DATA:")
print(df_test.head(), "\n")


# ===============================================================
# 2) TASK A CLEANING (Minimal — Only What Test Needs)
# ===============================================================

# Lowercase and strip object columns
obj_cols = df_test.select_dtypes(include=["object"]).columns
for c in obj_cols:
    df_test[c] = df_test[c].astype(str).str.strip().str.lower()

# --- Set-like parser (same logic as Task A) ---
def safe_parse_to_sorted_text(cell):
    try:
        obj = ast.literal_eval(cell)
        clean = sorted([str(x).strip().lower() for x in obj])
        return ", ".join(clean)
    except:
        return "none"

df_test["injury_type_classifications"] = df_test["injury_type_classifications"].apply(safe_parse_to_sorted_text)
df_test["rehabilitation_program_types"] = df_test["rehabilitation_program_types"].apply(safe_parse_to_sorted_text)

# --- Car damage severity cleaning ---
sev_map = {
    "none": "none",
    "minor": "minor",
    "moderate": "moderate",
    "severe": "severe",
    "total_loss": "total_loss"
}

df_test["car_damage_severity"] = df_test["car_damage_severity"].replace(sev_map)

sev_numeric = {
    "none": 0,
    "minor": 1,
    "moderate": 2,
    "severe": 3,
    "total_loss": 3
}

df_test["car_damage_severity_num"] = df_test["car_damage_severity"].map(sev_numeric).astype("Int64")

print("[TEST] AFTER CLEANING:")
print(df_test[[
    "client_id",
    "injury_type_classifications",
    "rehabilitation_program_types",
    "car_damage_severity",
    "car_damage_severity_num"
]], "\n")


# ===============================================================
# 3) TASK B – AIS / ISS SEVERITY MODEL
# ===============================================================

# AIS severity scores
ais_map = {
    "soft tissue injury": 1,
    "whiplash": 1,
    "burns and scalds": 2,
    "bone injury": 2,
    "facial injury": 2,
    "miscellaneous": 1,
    "psychological issues": 2,
    "internal injury": 3,
    "spinal injury": 3,
    "head injury": 3,
}

# ISS body region mapping
region_map = {
    "head injury": "head_neck",
    "whiplash": "head_neck",
    "psychological issues": "head_neck",
    "soft tissue injury": "external",
    "miscellaneous": "external",
    "burns and scalds": "external",
    "facial injury": "face",
    "internal injury": "abdomen_pelvis",
    "bone injury": "extremities_pelvic",
    "spinal injury": "extremities_pelvic"
}

def parse_inj_list(s):
    return [x.strip() for x in s.split(",") if x.strip()]

def compute_region_max(inj):
    reg = {}
    for i in parse_inj_list(inj):
        ais = ais_map.get(i, 1)
        region = region_map.get(i, "external")
        reg[region] = max(reg.get(region, 0), ais)
    return reg

def compute_iss(reg_dict):
    scores = list(reg_dict.values())
    if not scores:
        return 0
    top3 = sorted(scores, reverse=True)[:3]
    return sum([s*s for s in top3])

df_test["region_max_ais"] = df_test["injury_type_classifications"].apply(compute_region_max)
df_test["iss_like"] = df_test["region_max_ais"].apply(compute_iss)

# Severity index (non-data-driven)
def severity_index(row):
    score = (
        row["iss_like"] +
        0.05 * row["injury_duration_days"] +
        1.0 * row["car_damage_severity_num"] +
        0.0001 * row["general_damages"]
    )
    return round(score, 2)

df_test["severity_index"] = df_test.apply(severity_index, axis=1)


# ===============================================================
# 4) CORRELATION MATRIX
# ===============================================================
corr = df_test[["severity_index", "total_claim_amount", "injury_duration_days"]].corr()
print("[TEST] CORRELATION MATRIX:")
print(corr, "\n")


# ===============================================================
# 5) FINAL OUTPUT TABLE (For Report)
# ===============================================================
# Create a clean, human-readable summary table
df_test_report = df_test.copy()

# Round monetary values for neat display
df_test_report["total_claim_amount"] = df_test_report["total_claim_amount"].round(2)
df_test_report["general_damages"] = df_test_report["general_damages"].round(2)
df_test_report["severity_index"] = df_test_report["severity_index"].round(2)

# Select and order key columns
df_test_report = df_test_report[[
    "client_id",
    "injury_type_classifications",
    "iss_like",
    "severity_index",
    "injury_duration_days",
    "car_damage_severity",
    "general_damages",
    "total_claim_amount",
]]

# Sort by severity_index (highest first)
df_test_report = df_test_report.sort_values(by="severity_index", ascending=False).reset_index(drop=True)

# Rename columns for report (more readable, English)
df_test_report = df_test_report.rename(columns={
    "client_id": "Client ID",
    "injury_type_classifications": "Injury Types",
    "iss_like": "ISS-like Score",
    "severity_index": "Composite Severity Index",
    "injury_duration_days": "Injury Duration (days)",
    "car_damage_severity": "Car Damage Severity",
    "general_damages": "General Damages (£)",
    "total_claim_amount": "Total Claim Amount (£)",
})

print("[TEST] REPORT-READY SUMMARY TABLE:")
print(df_test_report.to_string(index=False))


# ===============================================================
# 6) VISUALISATIONS – CLEAN HISTOGRAMS AND BOXPLOTS
# ===============================================================

plot_cols = ["severity_index", "total_claim_amount", "injury_duration_days"]
pretty_names = {
    "severity_index": "Composite Severity Index",
    "total_claim_amount": "Total Claim Amount (£)",
    "injury_duration_days": "Injury Duration (days)",
}

for col in plot_cols:
    # Histogram
    plt.figure(figsize=(7, 4))
    df_test[col].hist(bins=5)
    plt.title(f"Distribution of {pretty_names[col]} (Test Samples)")
    plt.xlabel(pretty_names[col])
    plt.ylabel("Frequency")
    plt.grid(axis="y", alpha=0.3)
    plt.tight_layout()
    plt.show()

    # Boxplot
    plt.figure(figsize=(4, 5))
    df_test.boxplot(column=[col])
    plt.title(f"Boxplot of {pretty_names[col]} (Test Samples)")
    plt.ylabel(pretty_names[col])
    plt.tight_layout()
    plt.show()

# ===============================================
# TASK B – TEST SAMPLES (3 CASES) WITH AIS BREAKDOWN
# ===============================================
import ast

# Helper function for counting injuries
def count_injuries_from_str(injury_str):
    """
    Count number of injuries in a cleaned string like:
    'head injury, bone injury, soft tissue injury'
    """
    if not isinstance(injury_str, str):
        return 0
    if injury_str.strip().lower() == "none":
        return 0
    return len([x.strip() for x in injury_str.split(",") if x.strip()])

# Helper function for rule-based estimate (moved here to ensure it's defined)
BASE_AMOUNT = 1000             # fixed administrative + minimum claim
MEDICAL_PER_DAY = 50           # cost per day of injury
SEVERITY_PER_INJURY = 2000     # cost per injury type
HOSPITAL_VISIT_COST = 500
HOSPITAL_ADMISSION_COST = 3000
REHAB_COMPLETED_COST = 1000
VEHICLE_DAMAGE_MULTIPLIER = 1500  # * car_damage_severity_num

def rule_based_estimate(row: pd.Series) -> float:
    """
    Rule-based total claim estimator following the assignment's
    illustrative parameterisation.
    """
    base = BASE_AMOUNT
    medical = MEDICAL_PER_DAY * row["injury_duration_days"]
    severity = SEVERITY_PER_INJURY * row["num_injuries"]

    # Component costs
    comp = 0.0
    if row.get("hospital_visit_required", 0) == 1:
        comp += HOSPITAL_VISIT_COST
    if row.get("hospital_admission_required", 0) == 1:
        comp += HOSPITAL_ADMISSION_COST
    if row.get("rehabilitation_completed", 0) == 1:
        comp += REHAB_COMPLETED_COST

    # Vehicle damage
    veh = VEHICLE_DAMAGE_MULTIPLIER * row["car_damage_severity_num"]

    # General damages directly added
    gen = row["general_damages"]

    return base + medical + severity + comp + veh + gen


# 1) Test dataset
task_b_test_data = {
    "client_id": [1.0, 6.0, 8.0],
    "claim_value_category": [1.0, 2.0, 2.0],
    "injury_duration_days": [33.0, 44.0, 42.0],
    "hospital_visit_required": [1.0, 1.0, 1.0],
    "medical_care_sought": [1.0, 1.0, 1.0],
    "hospital_admission_required": [0.0, 0.0, 1.0],
    "currently_unemployed_due_to_injury": [0.0, 1.0, 0.0],
    "work_absence_duration_days": [2.0, 12.0, 0.0],
    "work_absence_required": [0.0, 1.0, 0.0],
    "rehabilitation_recommended": [0.0, 1.0, 1.0],
    "rehabilitation_completed": [0.0, 1.0, 1.0],
    "minor_claimant_at_initial_assessment": [0.0, 0.0, 0.0],
    "defendant_title_code": [0.0, 1.0, 0.0],
    "liability_admission_status": ["A", "A", "A"],
    "liability_type": ["motor", "motor", "motor"],
    "claimant_age_at_incident": [55.0, 31.0, 47.0],
    "liability_denial_reasons": ["", "", ""],
    "claim_rejection_code": ["", "", ""],
    "claims_management_exit_notes": ["", "", ""],
    "claims_resolution_exit_notes": ["", "", ""],
    "claims_resolution_closure_code": ["", "", ""],
    "rental_vehicle_expense": [0.0, 0.0, 0.0],
    "home_care_services_cost": [0.0, 0.0, 0.0],
    "employment_disadvantage_compensation": [0.0, 0.0, 0.0],
    "career_satisfaction_loss_compensation": [0.0, 0.0, 0.0],
    "asset_utility_loss_compensation": [0.0, 0.0, 0.0],
    "medical_treatment_costs": [125.32, 250.73, 500.12],
    "general_damages": [6056.74, 14563.83, 24008.63],
    "insurance_deductible_amount": [50.0, 100.0, 0.0],
    "car_damage_severity": ["none", "minor", "severe"],
    "total_claim_amount": [6232.06, 14914.56, 24508.75],
    "injury_type_classifications": [
        "{'Internal Injury', 'Psychological Issues'}",
        "{'Facial Injury', 'Psychological Issues', 'Soft Tissue Injury'}",
        "{'Head Injury', 'Miscellaneous', 'Soft Tissue Injury'}",
    ],
    "rehabilitation_program_types": [
        "{'Unknown'}",
        "{'exercises', 'pain relief'}",
        "{'miscellaneous', 'scans and imaging', 'surgery'}",
    ],
    "claimant_current_age_years": [55.0, 31.0, 47.0],
    "medical_attention_delay_days": [0.0, 0.0, 0.0],
    "emergency_services_attended": ["none", "ambulance", "ambulance_and_police"],
}

df_test = pd.DataFrame(task_b_test_data)

print("[TEST] RAW DATA:")
print(df_test.head(), "\n")

# 2) Minimal cleaning (Task A ile uyumlu)

# lowercase + strip
obj_cols = df_test.select_dtypes(include=["object"]).columns
for c in obj_cols:
    df_test[c] = df_test[c].astype(str).str.strip().str.lower()

# set-like parser → clean string
def safe_parse_to_sorted_text(cell):
    try:
        obj = ast.literal_eval(cell)
        clean = sorted([str(x).strip().lower() for x in obj])
        return ", ".join(clean)
    except:
        return "none"

df_test["injury_type_classifications"] = df_test["injury_type_classifications"].apply(safe_parse_to_sorted_text)
df_test["rehabilitation_program_types"] = df_test["rehabilitation_program_types"].apply(safe_parse_to_sorted_text)

# car_damage_severity + numeric
sev_map = {
    "none": "none",
    "minor": "minor",
    "moderate": "moderate",
    "severe": "severe",
    "total_loss": "total_loss",
}
df_test["car_damage_severity"] = df_test["car_damage_severity"].replace(sev_map)

sev_numeric = {
    "none": 0,
    "minor": 1,
    "moderate": 2,
    "severe": 3,
    "total_loss": 3,
}
df_test["car_damage_severity_num"] = df_test["car_damage_severity"].map(sev_numeric).astype("Int64")

# Add num_injuries to df_test before applying AIS/ISS scoring
df_test["num_injuries"] = df_test["injury_type_classifications"].apply(count_injuries_from_str)

# Calculate rule-based estimate for df_test before applying AIS/ISS scoring
df_test["rule_based_estimate"] = df_test.apply(rule_based_estimate, axis=1)

print("[TEST] AFTER CLEANING:")
print(df_test[[
    "client_id",
    "injury_type_classifications",
    "rehabilitation_program_types",
    "car_damage_severity",
    "car_damage_severity_num",
    "num_injuries",
    "rule_based_estimate"
]], "\n")

# 3) Aynı AIS / ISS / severity modelini test verisine uygula
df_test_scored = apply_injury_severity_scoring(df_test)

# 4) REPORT TABLOSU (AIS kolonuyla birlikte)
df_test_report = df_test_scored.copy()
df_test_report["total_claim_amount"] = df_test_report["total_claim_amount"].round(2)
df_test_report["general_damages"] = df_test_report["general_damages"].round(2)
df_test_report["severity_index"] = df_test_report["severity_index"].round(2)

df_test_report = df_test_report[[
    "client_id",
    "injury_type_classifications",
    "ais_scores_list",
    "iss_like",
    "severity_index",
    "injury_duration_days",
    "car_damage_severity",
    "general_damages",
    "total_claim_amount",
]]

df_test_report = df_test_report.rename(columns={
    "client_id": "Client ID",
    "injury_type_classifications": "Injury Types",
    "ais_scores_list": "AIS Scores",
    "iss_like": "ISS-like Score",
    "severity_index": "Composite Severity Index",
    "injury_duration_days": "Injury Duration (days)",
    "car_damage_severity": "Car Damage Severity",
    "general_damages": "General Damages (£)",
    "total_claim_amount": "Total Claim Amount (£)",
})

df_test_report = df_test_report.sort_values(by="Composite Severity Index", ascending=False).reset_index(drop=True)

print("\n[TEST] REPORT-READY SUMMARY TABLE (WITH AIS):")
print(df_test_report.to_string(index=False))

# 5) AIS BREAKDOWN TABLE – HER INJURY \u0130\u00c7\u0130N TEK TEK A\u00c7IKLAMA

rows = []
for _, row in df_test_scored.iterrows():
    labels = parse_injury_labels(row["injury_type_classifications"])
    for lab in labels:
        ais, region = injury_to_ais_and_region(lab)
        rows.append({
            "Client ID": row["client_id"],
            "Injury Label": lab,
            "AIS Score": ais,
            "Body Region": region,
        })

ais_breakdown_table = pd.DataFrame(rows)
print("\n[TEST] AIS BREAKDOWN TABLE:")
print(ais_breakdown_table.to_string(index=False))

"""# **TASK C**"""

def count_injuries_from_str(injury_str):
    """
    Count number of injuries in a cleaned string like:
    'head injury, bone injury, soft tissue injury'
    """
    if not isinstance(injury_str, str):
        return 0
    if injury_str.strip().lower() == "none":
        return 0
    return len([x.strip() for x in injury_str.split(",") if x.strip()])

### **PART 0 – Environment Preparation (Main & Test Set, auxiliary functions)** ###

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# -------------------------------------------------------------------
# Main dataset reference (data frame created by Task B)
# -------------------------------------------------------------------
# If I used df_b in Task B:
df_main = df_b.copy()



# -------------------------------------------------------------------
#Test dataset (3 samples) – df_test which we used in Task B
# -------------------------------------------------------------------
df_test = df_test.copy()

# -------------------------------------------------------------------
# Assistant: injury list & num_injuries
# -------------------------------------------------------------------
def parse_injury_labels(injury_str: str) -> list[str]:
    if not isinstance(injury_str, str):
        return []
    if injury_str.strip().lower() == "none":
        return []
    return [x.strip() for x in injury_str.split(",") if x.strip()]

# If num_injuries is not there, let's add it / if it is, let's update it
df_main["num_injuries"] = df_main["injury_type_classifications"].apply(count_injuries_from_str)
# The 'num_injuries' for df_test_scored is now calculated in mdUe0TmqsJP2
# The 'rule_based_estimate' for df_test_scored is now calculated in mdUe0TmqsJP2

# -------------------------------------------------------------------
# Car damage severity numeric mapping (just in case)
# -------------------------------------------------------------------
sev_numeric = {
    "none": 0,
    "minor": 1,
    "moderate": 2,
    "severe": 3,
    "total_loss": 3
}

if "car_damage_severity_num" not in df_main.columns:
    df_main["car_damage_severity_num"] = df_main["car_damage_severity"].map(sev_numeric).astype("Int64")

# The 'car_damage_severity_num' for df_test is now calculated in mdUe0TmqsJP2

print("[TaskC] Main shape:", df_main.shape)
print("[TaskC] Test shape:", df_test.shape)



# -------------------------------
# 3.1 – Correlation analysis
# -------------------------------
corr_features = [
    "total_claim_amount",
    "general_damages",
    "medical_treatment_costs",
    "injury_duration_days",
    "num_injuries",
    "car_damage_severity_num",
    "rehabilitation_recommended",
    "rehabilitation_completed",
    "iss_like",
    "severity_index",
    "claim_value_category",
]

corr_features = [c for c in corr_features if c in df_main.columns]

corr_matrix = df_main[corr_features].corr()

print("\n[TaskC] Correlation matrix with total_claim_amount:")
print(corr_matrix["total_claim_amount"].sort_values(ascending=False))

# Basit heatmap (istenirse)
plt.figure(figsize=(8,6))
plt.imshow(corr_matrix, cmap="Blues")
plt.colorbar(label="Correlation")
plt.xticks(range(len(corr_features)), corr_features, rotation=45, ha="right")
plt.yticks(range(len(corr_features)), corr_features)
plt.title("Correlation Matrix – Key Predictors vs Total Claim")
plt.tight_layout()
plt.show()



# -------------------------------
# 3.1 – Key relationship plots
# -------------------------------
plot_pairs = [
    ("injury_duration_days", "Total Claim vs Injury Duration"),
    ("general_damages", "Total Claim vs General Damages"),
    ("severity_index", "Total Claim vs Composite Severity Index"),
]

for x_col, title in plot_pairs:
    if x_col not in df_main.columns:
        continue

    plt.figure(figsize=(7,5))
    plt.scatter(df_main[x_col], df_main["total_claim_amount"], alpha=0.4)
    plt.xlabel(x_col.replace("_", " ").title())
    plt.ylabel("Total Claim Amount (£)")
    plt.title(title)
    plt.tight_layout()
    plt.show()

# if there is rehab: boxplot – rehab completed vs claim
if "rehabilitation_completed" in df_main.columns:
    plt.figure(figsize=(6,4))
    df_main.boxplot(column="total_claim_amount", by="rehabilitation_completed")
    plt.title("Total Claim Amount by Rehabilitation Completion")
    plt.suptitle("")
    plt.xlabel("Rehabilitation Completed (0 = No, 1 = Yes)")
    plt.ylabel("Total Claim Amount (£)")
    plt.tight_layout()
    plt.show()



# ======================================================
# 3.2(a) RULE-BASED BASELINE MODEL
# ======================================================

# The parameters are adapted from the example in the assignment:
BASE_AMOUNT = 1000             # fixed administrative + minimum claim
MEDICAL_PER_DAY = 50           # cost per day of injury
SEVERITY_PER_INJURY = 2000     # cost per injury type
HOSPITAL_VISIT_COST = 500
HOSPITAL_ADMISSION_COST = 3000
REHAB_COMPLETED_COST = 1000
VEHICLE_DAMAGE_MULTIPLIER = 1500  # * car_damage_severity_num


def rule_based_estimate(row: pd.Series) -> float:
    """
    Rule-based total claim estimator following the assignment's
    illustrative parameterisation.
    """
    base = BASE_AMOUNT
    medical = MEDICAL_PER_DAY * row["injury_duration_days"]
    severity = SEVERITY_PER_INJURY * row["num_injuries"]

    # Component costs
    comp = 0.0
    if row.get("hospital_visit_required", 0) == 1:
        comp += HOSPITAL_VISIT_COST
    if row.get("hospital_admission_required", 0) == 1:
        comp += HOSPITAL_ADMISSION_COST
    if row.get("rehabilitation_completed", 0) == 1:
        comp += REHAB_COMPLETED_COST

    # Vehicle damage
    veh = VEHICLE_DAMAGE_MULTIPLIER * row["car_damage_severity_num"]

    # General damages directly added
    gen = row["general_damages"]

    return base + medical + severity + comp + veh + gen


# Applying: main + test set
df_main["rule_based_estimate"] = df_main.apply(rule_based_estimate, axis=1)
df_test["rule_based_estimate"] = df_test.apply(rule_based_estimate, axis=1)

# Performance metric function
def evaluate_model(y_true, y_pred, label="model"):
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)
    print(f"\n[{label}] MAE  : {mae:,.2f}")
    print(f"[{label}] RMSE : {rmse:,.2f}")
    print(f"[{label}] R^2  : {r2:,.3f}")
    return mae, rmse, r2

print("\n[TaskC] Rule-based model performance on MAIN dataset:")
evaluate_model(df_main["total_claim_amount"], df_main["rule_based_estimate"], label="Rule-based (main)")

print("\n[TaskC] Rule-based model performance on TEST samples:")
evaluate_model(df_test["total_claim_amount"], df_test["rule_based_estimate"], label="Rule-based (test)")


print("\n[TaskC] Rule-based vs actual (TEST samples):")
print(df_test[["client_id", "total_claim_amount", "rule_based_estimate"]])



# ======================================================
# 3.2(b) MACHINE LEARNING MODEL – LINEAR REGRESSION
# ======================================================

# Candidate features (use if available, skip automatically if not)
candidate_features = [
    "injury_duration_days",
    "num_injuries",
    "hospital_visit_required",
    "hospital_admission_required",
    "rehabilitation_recommended",
    "rehabilitation_completed",
    "car_damage_severity_num",
    "general_damages",
    "medical_treatment_costs",
    "insurance_deductible_amount",
    "claim_value_category",
    "currently_unemployed_due_to_injury",
    "work_absence_duration_days",
    "claimant_age_at_incident",
    "medical_attention_delay_days",
    "iss_like",
    "severity_index",
]

feature_cols = [c for c in candidate_features if c in df_main.columns]
print("\n[TaskC] Features used in ML model:", feature_cols)

X = df_main[feature_cols].copy()
y = df_main["total_claim_amount"].astype(float)

# Train / test split (portföy üzerinde internal validation)
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("[TaskC] Train size:", X_train.shape, "Validation size:", X_val.shape)

# -------------------------
# Train Linear Regression
# -------------------------
lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)

# predicts
y_val_pred = lin_reg.predict(X_val)

print("\n[TaskC] Linear regression performance (VALIDATION set):")
evaluate_model(y_val, y_val_pred, label="LinearRegression (val)")

# Final performance on the entire main dataset
y_main_pred = lin_reg.predict(X)
print("\n[TaskC] Linear regression performance (FULL MAIN data):")
evaluate_model(y, y_main_pred, label="LinearRegression (full main)")



# ==========================================
# 3.2(c) TEST SAMPLES – PREDICTION COMPARISON
# ==========================================

# Use the same feature set for the test set as well
X_test = df_test_scored[feature_cols].copy()
y_test_true = df_test_scored["total_claim_amount"].astype(float)

# ML predictions
y_test_pred_ml = lin_reg.predict(X_test)

df_test_results = df_test_scored.copy()
df_test_results["pred_rule_based"] = df_test_scored["rule_based_estimate"]
df_test_results["pred_linear_reg"] = y_test_pred_ml

#(signed error)
df_test_results["err_rule_based"] = df_test_results["pred_rule_based"] - df_test_results["total_claim_amount"]
df_test_results["err_linear_reg"] = df_test_results["pred_linear_reg"] - df_test_results["total_claim_amount"]


print("\n[TaskC] TEST SET – Model comparison table:")
print(df_test_results[[
    "client_id",
    "total_claim_amount",
    "pred_rule_based",
    "pred_linear_reg",
    "err_rule_based",

]].to_string(index=False))

# ---- Performans metrikleri ----
print("\n[TaskC] Rule-based performance on TEST samples:")
evaluate_model(y_test_true, df_test_results["pred_rule_based"], label="Rule-based (test)")

print("\n[TaskC] Linear regression performance on TEST samples:")
evaluate_model(y_test_true, y_test_pred_ml, label="LinearRegression (test)")

from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor

# ================================
# 3 additional Models: RF, GBM, KNN
# ================================

rf = RandomForestRegressor(n_estimators=300, random_state=42)
gbm = GradientBoostingRegressor(random_state=42)
knn = KNeighborsRegressor(n_neighbors=5)

# Fit models on MAIN dataset
rf.fit(X_train, y_train)
gbm.fit(X_train, y_train)
knn.fit(X_train, y_train)

# Predictions on TEST dataset
test_pred_rf = rf.predict(X_test)
test_pred_gbm = gbm.predict(X_test)
test_pred_knn = knn.predict(X_test)

# Save into results table
df_test_results["pred_rf"] = test_pred_rf
df_test_results["pred_gbm"] = test_pred_gbm
df_test_results["pred_knn"] = test_pred_knn

df_test_results["err_rf"] = df_test_results["pred_rf"] - df_test_results["total_claim_amount"]
df_test_results["err_gbm"] = df_test_results["pred_gbm"] - df_test_results["total_claim_amount"]
df_test_results["err_knn"] = df_test_results["pred_knn"] - df_test_results["total_claim_amount"]

# Print comparison
print("\n[TaskC] TEST SET – FULL MODEL COMPARISON:")
print(df_test_results[[
    "client_id",
    "total_claim_amount",
    "pred_rule_based",
    "pred_linear_reg",
    "pred_rf",
    "pred_gbm",
    "pred_knn",
    "err_rule_based",
    "err_linear_reg",
    "err_rf",
    "err_gbm",
    "err_knn"
]].to_string(index=False))

# Evaluate performance
print("\n[TaskC] PERFORMANCE COMPARISON (TEST SET):")
evaluate_model(y_test_true, y_test_pred_ml, label="Linear Regression")
evaluate_model(y_test_true, test_pred_rf, label="Random Forest")
evaluate_model(y_test_true, test_pred_gbm, label="Gradient Boosting")
evaluate_model(y_test_true, test_pred_knn, label="KNN Regression")



# ============================================================
# EXTRA VISUALISATIONS FOR REPORT (TASK B / C)
# ============================================================
import matplotlib.pyplot as plt
import seaborn as sns

sns.set(style="whitegrid", context="talk")


# -----------------------------
# Helper: Simple 3-band severity from iss_like
# -----------------------------
def simple_severity_band_from_iss(iss_val: int) -> str:
    """
    Simple 3-level band:
      0–8   → minor
      9–15  → moderate
      ≥16   → serious
    (Report için minor / moderate / serious şeklinde özet.)
    """
    if pd.isna(iss_val):
        return "unknown"
    if iss_val <= 8:
        return "minor"
    elif iss_val <= 15:
        return "moderate"
    else:
        return "serious"


# -----------------------------
# 1) Severity Band Construction (iss_like → minor/moderate/serious)
# -----------------------------
def plot_severity_bands(df: pd.DataFrame) -> None:
    if "iss_like" not in df.columns:
        print("[VIS] 'iss_like' column not found – skipping severity bands.")
        return

    if "severity_band_simple" not in df.columns:
        df["severity_band_simple"] = df["iss_like"].apply(simple_severity_band_from_iss)

    counts = df["severity_band_simple"].value_counts()
    pct = (counts / counts.sum() * 100).round(1)
    print("\n[VIS] Severity bands (simple, %):")
    print(pct)

    # Bar chart
    plt.figure(figsize=(7, 6))
    counts.plot(kind="bar")
    plt.xlabel("Severity Band (ISS-based)")
    plt.ylabel("Number of Claims")
    plt.title("Severity Bands from ISS-like Score (Minor / Moderate / Serious)")
    plt.tight_layout()
    plt.show()

    # Pie chart
    plt.figure(figsize=(7, 7))
    plt.pie(counts.values, labels=counts.index, autopct="%1.1f%%", startangle=90)
    plt.title("Severity Band Distribution (ISS-based)")
    plt.tight_layout()
    plt.show()





# -----------------------------
# 3) Distribution of Total Claim Amount
#    (Histogram + KDE + Log-scale)
# -----------------------------
def plot_total_claim_distribution(df: pd.DataFrame) -> None:
    col = "total_claim_amount"
    if col not in df.columns:
        print("[VIS] 'total_claim_amount' not found.")
        return

    series = df[col].dropna()
    print(f"\n[VIS] {col} – min={series.min():,.2f}, max={series.max():,.2f}")
    print(series.describe())

    # Linear scale
    plt.figure(figsize=(8, 6))
    sns.histplot(series, bins=30, kde=True)
    plt.xlabel("Total Claim Amount (£)")
    plt.ylabel("Frequency")
    plt.title("Distribution of Total Claim Amount (Linear Scale)")
    plt.tight_layout()
    plt.show()

    # Log10 scale
    positive = series[series > 0]
    plt.figure(figsize=(8, 6))
    sns.histplot(np.log10(positive), bins=30, kde=True)
    plt.xlabel("log10(Total Claim Amount £)")
    plt.ylabel("Frequency")
    plt.title("Distribution of Total Claim Amount (Log10 Scale)")
    plt.tight_layout()
    plt.show()


# -----------------------------
# 4) Claim Cost Breakdown – Boxplots
#    (general_damages vs medical_treatment_costs)
# -----------------------------
def plot_claim_cost_breakdown(df: pd.DataFrame) -> None:
    cols = [c for c in ["general_damages", "medical_treatment_costs"] if c in df.columns]
    if len(cols) < 2:
        print("[VIS] Need both 'general_damages' and 'medical_treatment_costs'.")
        return

    long_df = df[cols].melt(var_name="Cost Type", value_name="Amount")

    plt.figure(figsize=(8, 6))
    sns.boxplot(data=long_df, x="Cost Type", y="Amount")
    plt.yscale("log")
    plt.ylabel("Amount (£, log scale)")
    plt.xlabel("")
    plt.title("Claim Cost Breakdown – General Damages vs Medical Treatment Costs")
    plt.tight_layout()
    plt.show()


# -----------------------------
# 5) Injury Duration Distribution
# -----------------------------
def plot_injury_duration_distribution(df: pd.DataFrame) -> None:
    col = "injury_duration_days"
    if col not in df.columns:
        print("[VIS] 'injury_duration_days' not found.")
        return

    s = df[col].dropna()
    print(f"\n[VIS] injury_duration_days – mean={s.mean():.2f}, median={s.median():.2f}, max={s.max():.2f}")

    plt.figure(figsize=(8, 6))
    sns.histplot(s, bins=30, kde=True)
    plt.xlabel("Injury Duration (days)")
    plt.ylabel("Number of Claims")
    plt.title("Distribution of Injury Duration")
    plt.tight_layout()
    plt.show()


# -----------------------------
# 6) Medical Attention Delay Distribution
# -----------------------------
def plot_medical_attention_delay(df: pd.DataFrame) -> None:
    col = "medical_attention_delay_days"
    if col not in df.columns:
        print("[VIS] 'medical_attention_delay_days' not found.")
        return

    s = df[col].dropna()
    print(f"\n[VIS] medical_attention_delay_days – median={s.median():.2f}, max={s.max():.2f}")

    plt.figure(figsize=(8, 6))
    sns.histplot(s, bins=30, kde=False)
    plt.xlabel("Delay Before Medical Attention (days)")
    plt.ylabel("Number of Claims")
    plt.title("Distribution of Delay in Seeking Medical Attention")
    plt.tight_layout()
    plt.show()


# -----------------------------
# 7) Top Injury Types (Bar Chart from exploded labels)
# -----------------------------
def plot_top_injury_types(df: pd.DataFrame, top_n: int = 10) -> None:
    col = "injury_type_classifications"
    if col not in df.columns:
        print("[VIS] 'injury_type_classifications' not found.")
        return

    # parse_injury_labels zaten yukarıda tanımlı (Task B)
    all_labels = df[col].apply(parse_injury_labels).explode().dropna()
    all_labels = all_labels.str.strip().str.lower()
    total = len(all_labels)
    counts = all_labels.value_counts().head(top_n)
    pct = (counts / total * 100).round(1)

    print(f"\n[VIS] Top {top_n} injury types (% of all labels):")
    print(pct)

    plt.figure(figsize=(9, 6))
    sns.barplot(x=counts.values, y=counts.index)
    plt.xlabel("Number of Mentions")
    plt.ylabel("Injury Type")
    plt.title(f"Top {top_n} Injury Types")
    plt.tight_layout()
    plt.show()


# -----------------------------
# 8) Severity Bands (Pie + Stacked Bar, simple bands)
# -----------------------------
def plot_severity_band_distribution(df: pd.DataFrame) -> None:
    if "severity_band_simple" not in df.columns:
        if "iss_like" not in df.columns:
            print("[VIS] No 'iss_like' / 'severity_band_simple'.")
            return
        df["severity_band_simple"] = df["iss_like"].apply(simple_severity_band_from_iss)

    counts = df["severity_band_simple"].value_counts()
    pct = (counts / counts.sum() * 100).round(1)
    print("\n[VIS] Simple severity band %:")
    print(pct)

    # Pie
    plt.figure(figsize=(7, 7))
    plt.pie(counts.values, labels=counts.index, autopct="%1.1f%%", startangle=90)
    plt.title("Severity Bands (Minor / Moderate / Serious)")
    plt.tight_layout()
    plt.show()

    # Stacked bar
    plt.figure(figsize=(7, 2))
    bottom = 0
    for band, v in pct.items():
        plt.barh(["Claims"], [v], left=bottom, label=f"{band} ({v:.1f}%)")
        bottom += v
    plt.xlabel("Percentage of Claims (%)")
    plt.title("Severity Band Mix – Stacked Bar")
    plt.legend(bbox_to_anchor=(1.05, 1), loc="upper left")
    plt.tight_layout()
    plt.show()


# -----------------------------
# 9) Rehab Funnel (Recommended → Completed)
# -----------------------------
def plot_rehab_funnel(df: pd.DataFrame) -> None:
    if "rehabilitation_recommended" not in df.columns or "rehabilitation_completed" not in df.columns:
        print("[VIS] Rehab columns not found.")
        return

    rec_rate = df["rehabilitation_recommended"].mean() * 100
    comp_rate = df["rehabilitation_completed"].mean() * 100

    print(f"\n[VIS] Rehab recommended: {rec_rate:.1f}%")
    print(f"[VIS] Rehab completed  : {comp_rate:.1f}%")

    labels = ["Rehab Recommended", "Rehab Completed"]
    values = [rec_rate, comp_rate]

    plt.figure(figsize=(6, 6))
    sns.barplot(x=labels, y=values)
    plt.ylim(0, 100)
    plt.ylabel("Percentage of Claimants (%)")
    plt.title("Rehabilitation Funnel – Recommended vs Completed")
    for i, v in enumerate(values):
        plt.text(i, v + 1, f"{v:.1f}%", ha="center")
    plt.tight_layout()
    plt.show()


# -----------------------------
# 10) Duration by Rehab Status (Boxplot)
# -----------------------------
def plot_duration_by_rehab_status(df: pd.DataFrame) -> None:
    if "rehabilitation_completed" not in df.columns or "injury_duration_days" not in df.columns:
        print("[VIS] Missing rehab/duration columns.")
        return

    temp = df[["rehabilitation_completed", "injury_duration_days"]].copy()
    temp["rehab_status"] = temp["rehabilitation_completed"].map({1: "Completed", 0: "Not Completed"})

    plt.figure(figsize=(8, 6))
    sns.boxplot(data=temp, x="rehab_status", y="injury_duration_days")
    plt.xlabel("Rehabilitation Status")
    plt.ylabel("Injury Duration (days)")
    plt.title("Injury Duration by Rehabilitation Completion")
    plt.tight_layout()
    plt.show()


# -----------------------------
# 11) Scatter: Injury Duration vs Total Claim Amount
# -----------------------------
def plot_duration_vs_total_claim(df: pd.DataFrame) -> None:
    if "injury_duration_days" not in df.columns or "total_claim_amount" not in df.columns:
        print("[VIS] Missing duration/claim columns.")
        return

    s = df[["injury_duration_days", "total_claim_amount"]].dropna()
    r = s.corr().iloc[0, 1]
    print(f"\n[VIS] Corr(injury_duration_days, total_claim_amount) = {r:.3f}")

    plt.figure(figsize=(8, 6))
    sns.regplot(
        data=s,
        x="injury_duration_days",
        y="total_claim_amount",
        scatter_kws={"alpha": 0.4},
        line_kws={"linewidth": 2},
    )
    plt.xlabel("Injury Duration (days)")
    plt.ylabel("Total Claim Amount (£)")
    plt.title("Injury Duration vs Total Claim Amount")
    plt.tight_layout()
    plt.show()


# -----------------------------
# 12) Outlier Drivers – Rental vs Home Care Costs
# -----------------------------
def plot_outlier_drivers(df: pd.DataFrame) -> None:
    cols = []
    if "rental_vehicle_expense" in df.columns:
        cols.append("rental_vehicle_expense")
    if "home_care_services_cost" in df.columns:
        cols.append("home_care_services_cost")

    if not cols:
        print("[VIS] No outlier driver cost columns found.")
        return

    long_df = df[cols].melt(var_name="Cost Type", value_name="Amount")

    plt.figure(figsize=(8, 6))
    sns.boxplot(data=long_df, x="Cost Type", y="Amount")
    plt.yscale("log")
    plt.xlabel("")
    plt.ylabel("Amount (£, log scale)")
    plt.title("Outlier Drivers – Rental Vehicle vs Home Care Costs")
    plt.tight_layout()
    plt.show()


# -----------------------------
# 13) Conceptual Diagram
# -----------------------------
def plot_conceptual_diagram() -> None:
    plt.figure(figsize=(10, 3))
    plt.axis("off")

    plt.text(0.1, 0.5, "Longer Injury Duration", fontsize=14,
             bbox=dict(boxstyle="round", fc="white", ec="black"))
    plt.text(0.45, 0.5, "More Negotiation /\nCare Services\n(e.g. Rehab, Home Care)", fontsize=14,
             bbox=dict(boxstyle="round", fc="white", ec="black"))
    plt.text(0.8, 0.5, "Higher Total\nClaim Cost", fontsize=14,
             bbox=dict(boxstyle="round", fc="white", ec="black"))

    plt.annotate("", xy=(0.38, 0.5), xytext=(0.23, 0.5),
                 arrowprops=dict(arrowstyle="->", lw=2))
    plt.annotate("", xy=(0.73, 0.5), xytext=(0.58, 0.5),
                 arrowprops=dict(arrowstyle="->", lw=2))

    plt.title("Conceptual Link: Duration → Services/Negotiation → Total Claim Cost", pad=20)
    plt.tight_layout()
    plt.show()


# -----------------------------
# Master function – run all visuals
# -----------------------------
def run_all_visuals(df: pd.DataFrame) -> None:
    """
    Convenience wrapper:
    run_all_visuals(df_main)  # or df_b
    """
    plot_severity_bands(df)

    plot_total_claim_distribution(df)
    plot_claim_cost_breakdown(df)
    plot_injury_duration_distribution(df)
    plot_medical_attention_delay(df)
    plot_top_injury_types(df, top_n=10)
    plot_severity_band_distribution(df)
    plot_rehab_funnel(df)
    plot_duration_by_rehab_status(df)
    plot_duration_vs_total_claim(df)
    plot_outlier_drivers(df)
    plot_conceptual_diagram()

run_all_visuals(df_main)